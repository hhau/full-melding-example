- Write up the two models from the melding paper and Anne's original paper
  - motivation will come from Anne's paper - model specifics more likely from the melding
- Interested in phi_2 of the phis (check the paper)

Practicalities:

WSRE: Now needs to be 2D?
  - `wf_mean` now a list of 2D vectors? Should work as is?
    - Absolutely not - need to adjust underlying KDEs to work for N-dim.

Stage one: ICU submodel
  - Sample stage one prior / posterior
    - Do we have an expression for the stage one prior?
    - Confirm that the prior is flat enough
      - plot samples / prior estimate
      - this is pretty much confirmed
      - this means we don't have to target p_{1}(phi, psi_{1}, y_{1}) / p_{1}(phi) in stage one, because the prior marginal has minimal difference
    - motivates considering the model one prior in stage two

Stage two: ICU + Severity submodel.
    - recode the stage two model in Stan
    - Sample original stage two (without kludges?)
    - estimate the prior marginal using `wsre`
    - decided what kind of pooling we would like to use
      - could do both? Linear 1/2 + log equal
    - sample the stage two prior (which includes the prior from model 1, the prior from model 2, the pooled prior, and the stage 2 posterior)
    - Compare and contrast!

NOTES: 
- phi_1 is tot.conf[1] and phi_2 is tot.conf[2]
- Given the plot in the paper - we should be interested in tot.conf[1]


TODO: This really needs to be faster

  - If the mixing properties are good, we can just run a nodes worth of chains for eight hours.
  - Probably worth profiling what exactly is slow here? 
    - I know the log pooled_prior term is slow, as is the log_sev_prior term. The `wsre` bit takes a long time to evaluate. Not sure I can speed that up?
  - Am I comparing this vs not doing the `wsre`? That also needs to be a comparison
  - Final plots of phi / psi_{2}? I can go back and filter psi_{1} if I wanted too as well I guess.

# I made a trivial edit on the hpc - lets see if rsync pulls it back

Tue Sep 17 11:11:33 2019
- 13500 samples takes 5:16 hours
- we can hence do a lot more in a job
- thinning is going to become important
  - not really, file sizes are still quite small.
- Lets see if we can write out an RDS file with the SLURM job id in it?
  - possible, perhaps unnecessary
- Be able to read and batch arbitrary number of RDS files
  - No problem, if we want to collapse over multiple simulations, we need to collapse over equal length chains first (just for array compatibility)
- We need to have some ESS diagnostics
  - True? I have yet to do this.
  - Actually, unless I want to go to all the effort of checking what the computational cost of a single wsre/no wsre sample is, then I just need to do enough for the reader to believe that it is "fair enough"
- phi_{a} is the only thing we should monitor for ESS
  - still driven by phi_{1}
- Still need to compare with / without wsre, consider recreating Fig 8 from the melding paper but just using log pooling, with/without wsre
  - easy enough.

Tue Sep 24 11:15:23 2019 check notebook for new things to do
